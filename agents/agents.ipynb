{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2a84802",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d02bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1108681f0>, search_kwargs={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)\n",
    "vectordb = Chroma.from_documents(documents, OllamaEmbeddings(model=\"llama3\"))\n",
    "retriever = vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bac692d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith_search'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(retriever, \"langsmith_search\",\n",
    "                                       \"Search for information about LangSmith. For any questions about LangSmith, you must stick to this.\")\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3be5c66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a04eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki, arxiv, retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48999712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w8/58g_7p917x5gg9zz13xng8g80000gn/T/ipykernel_30526/3244524374.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3\", base_url=\"http://localhost:11434\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3\", base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e651250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an ai that will use all of its best abilities to anwswer the user in an easy way\"),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f05d0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,  # or another agent type if needed\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c9142bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableAgent(runnable=AgentExecutor(verbose=True, tags=['openai-functions'], agent=OpenAIFunctionsAgent(llm=ChatOllama(model='llama3'), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/shikhar/Desktop/Code/langchain/venv/lib/python3.9/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)), Tool(name='langsmith_search', description='Search for information about LangSmith. For any questions about LangSmith, you must stick to this.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x1229c64c0>, retriever=VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1108681f0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x1229c6430>, retriever=VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1108681f0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))], prompt=ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x11065aa60>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessage(content='You are a helpful AI assistant.', additional_kwargs={}, response_metadata={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')]), output_parser=<class 'langchain.agents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser'>), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/shikhar/Desktop/Code/langchain/venv/lib/python3.9/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)), Tool(name='langsmith_search', description='Search for information about LangSmith. For any questions about LangSmith, you must stick to this.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x1229c64c0>, retriever=VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1108681f0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x1229c6430>, retriever=VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1108681f0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/shikhar/Desktop/Code/langchain/venv/lib/python3.9/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)), Tool(name='langsmith_search', description='Search for information about LangSmith. For any questions about LangSmith, you must stick to this.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x1229c64c0>, retriever=VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1108681f0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x1229c6430>, retriever=VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1108681f0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e9ca0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72c691ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangsmith!\n",
      "\n",
      "Langsmith is a fascinating topic! From what I understand, Langsmith refers to individuals or companies that specialize in creating and maintaining high-quality language processing models for various applications. These models can be used for tasks such as:\n",
      "\n",
      "1. Natural Language Processing (NLP): Langsmiths develop models that can comprehend and generate human-like text, enabling applications like chatbots, voice assistants, and text summarization.\n",
      "2. Machine Translation: Langsmiths create models that can translate texts from one language to another with high accuracy, helping bridge linguistic gaps between cultures and industries.\n",
      "3. Sentiment Analysis: Langsmiths design models that can analyze emotions and sentiment behind written or spoken language, enabling applications like customer feedback analysis, opinion mining, and emotional intelligence.\n",
      "\n",
      "Some notable Langsmith companies include:\n",
      "\n",
      "1. Google's Language Translation Team\n",
      "2. Microsoft's Language Understanding Intelligence (LUIS)\n",
      "3. Amazon's Alexa NLP team\n",
      "\n",
      "What would you like to know more about regarding Langsmith?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me something about langsmith',\n",
       " 'output': \"Langsmith!\\n\\nLangsmith is a fascinating topic! From what I understand, Langsmith refers to individuals or companies that specialize in creating and maintaining high-quality language processing models for various applications. These models can be used for tasks such as:\\n\\n1. Natural Language Processing (NLP): Langsmiths develop models that can comprehend and generate human-like text, enabling applications like chatbots, voice assistants, and text summarization.\\n2. Machine Translation: Langsmiths create models that can translate texts from one language to another with high accuracy, helping bridge linguistic gaps between cultures and industries.\\n3. Sentiment Analysis: Langsmiths design models that can analyze emotions and sentiment behind written or spoken language, enabling applications like customer feedback analysis, opinion mining, and emotional intelligence.\\n\\nSome notable Langsmith companies include:\\n\\n1. Google's Language Translation Team\\n2. Microsoft's Language Understanding Intelligence (LUIS)\\n3. Amazon's Alexa NLP team\\n\\nWhat would you like to know more about regarding Langsmith?\"}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Tell me something about langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "482afd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mIIIT Sonepat!\n",
      "\n",
      "Here's some information I've gathered:\n",
      "\n",
      "**About**\n",
      "Indian Institute of Information Technology (IIIT), Sonepat is a premier technical institute located in Sonepat, Haryana, India. It was established in 2014 as one of the five IIITs set up by the Ministry of Human Resource Development (MHRD) to promote excellence in education and research in the field of Information Technology.\n",
      "\n",
      "**Courses**\n",
      "IIIT Sonepat offers undergraduate programs in:\n",
      "\n",
      "1. Bachelor of Technology (BTech) in Computer Science and Engineering\n",
      "2. BTech in Electronics and Communication Engineering\n",
      "3. BTech in Mechanical Engineering\n",
      "\n",
      "They also offer Master's programs in:\n",
      "\n",
      "1. Master of Technology (MTech) in Computer Science and Engineering\n",
      "2. MTech in Information Technology\n",
      "\n",
      "**Facilities**\n",
      "The institute has state-of-the-art infrastructure, including:\n",
      "\n",
      "* Spacious classrooms and auditoriums\n",
      "* Modern laboratories with latest equipment and tools\n",
      "* High-speed internet connectivity\n",
      "* Library with a vast collection of books, journals, and online resources\n",
      "* Hostels for boys and girls with all amenities\n",
      "\n",
      "**Research**\n",
      "IIIT Sonepat is known for its research activities in areas like:\n",
      "\n",
      "1. Artificial Intelligence (AI) and Machine Learning (ML)\n",
      "2. Data Science and Analytics\n",
      "3. Cybersecurity and Network Security\n",
      "4. Internet of Things (IoT)\n",
      "\n",
      "**Campus Life**\n",
      "The campus life at IIIT Sonepat is vibrant, with various clubs and societies that cater to different interests, such as:\n",
      "\n",
      "1. Robotics Club\n",
      "2. Coding Club\n",
      "3. Cultural Club\n",
      "4. Sports Club\n",
      "\n",
      "**Placement**\n",
      "The institute has a strong placement record, with many students securing jobs in top companies like Google, Amazon, Microsoft, and more.\n",
      "\n",
      "I hope this information helps! Do you have any specific questions or would you like to know more about IIIT Sonepat?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me something about IIIT Sonepat',\n",
       " 'output': \"IIIT Sonepat!\\n\\nHere's some information I've gathered:\\n\\n**About**\\nIndian Institute of Information Technology (IIIT), Sonepat is a premier technical institute located in Sonepat, Haryana, India. It was established in 2014 as one of the five IIITs set up by the Ministry of Human Resource Development (MHRD) to promote excellence in education and research in the field of Information Technology.\\n\\n**Courses**\\nIIIT Sonepat offers undergraduate programs in:\\n\\n1. Bachelor of Technology (BTech) in Computer Science and Engineering\\n2. BTech in Electronics and Communication Engineering\\n3. BTech in Mechanical Engineering\\n\\nThey also offer Master's programs in:\\n\\n1. Master of Technology (MTech) in Computer Science and Engineering\\n2. MTech in Information Technology\\n\\n**Facilities**\\nThe institute has state-of-the-art infrastructure, including:\\n\\n* Spacious classrooms and auditoriums\\n* Modern laboratories with latest equipment and tools\\n* High-speed internet connectivity\\n* Library with a vast collection of books, journals, and online resources\\n* Hostels for boys and girls with all amenities\\n\\n**Research**\\nIIIT Sonepat is known for its research activities in areas like:\\n\\n1. Artificial Intelligence (AI) and Machine Learning (ML)\\n2. Data Science and Analytics\\n3. Cybersecurity and Network Security\\n4. Internet of Things (IoT)\\n\\n**Campus Life**\\nThe campus life at IIIT Sonepat is vibrant, with various clubs and societies that cater to different interests, such as:\\n\\n1. Robotics Club\\n2. Coding Club\\n3. Cultural Club\\n4. Sports Club\\n\\n**Placement**\\nThe institute has a strong placement record, with many students securing jobs in top companies like Google, Amazon, Microsoft, and more.\\n\\nI hope this information helps! Do you have any specific questions or would you like to know more about IIIT Sonepat?\"}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"tell me something about IIIT Sonepat\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2070a2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mA specific research paper!\n",
      "\n",
      "Paper 1605.08386 appears to be a preprint titled \"Attention Is All You Need\" by Ashish Vaswani, Noam Shazeer, Niki Parmar, Dan Zhang, Michael Mathieu, Sanjeev Gupta, Luke Henderson, Vikram Ramanujan, and Andrew M. Dai.\n",
      "\n",
      "Published in May 2016, this paper introduced the Transformer model, a groundbreaking neural network architecture for machine translation that revolutionized the field of natural language processing (NLP).\n",
      "\n",
      "The authors proposed a novel approach to sequence-to-sequence learning, abandoning traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in favor of self-attention mechanisms. This allowed the model to directly focus on relevant parts of the input sequence, eliminating the need for recurrent or convolutions.\n",
      "\n",
      "The paper's main contributions include:\n",
      "\n",
      "1. A new Transformer architecture that replaces RNNs and CNNs with self-attention mechanisms.\n",
      "2. The use of multi-head attention, which allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "\n",
      "The authors demonstrated the effectiveness of their approach on multiple machine translation benchmarks, achieving state-of-the-art results at the time. Since then, the Transformer architecture has become a standard component in many NLP applications, including language translation, text generation, and question answering.\n",
      "\n",
      "Overall, Paper 1605.08386 is a seminal work that has had a profound impact on the field of NLP, enabling new levels of efficiency, scalability, and accuracy in many natural language processing tasks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the paper 1605.08386 about?\",\n",
       " 'output': 'A specific research paper!\\n\\nPaper 1605.08386 appears to be a preprint titled \"Attention Is All You Need\" by Ashish Vaswani, Noam Shazeer, Niki Parmar, Dan Zhang, Michael Mathieu, Sanjeev Gupta, Luke Henderson, Vikram Ramanujan, and Andrew M. Dai.\\n\\nPublished in May 2016, this paper introduced the Transformer model, a groundbreaking neural network architecture for machine translation that revolutionized the field of natural language processing (NLP).\\n\\nThe authors proposed a novel approach to sequence-to-sequence learning, abandoning traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in favor of self-attention mechanisms. This allowed the model to directly focus on relevant parts of the input sequence, eliminating the need for recurrent or convolutions.\\n\\nThe paper\\'s main contributions include:\\n\\n1. A new Transformer architecture that replaces RNNs and CNNs with self-attention mechanisms.\\n2. The use of multi-head attention, which allows the model to jointly attend to information from different representation subspaces at different positions.\\n\\nThe authors demonstrated the effectiveness of their approach on multiple machine translation benchmarks, achieving state-of-the-art results at the time. Since then, the Transformer architecture has become a standard component in many NLP applications, including language translation, text generation, and question answering.\\n\\nOverall, Paper 1605.08386 is a seminal work that has had a profound impact on the field of NLP, enabling new levels of efficiency, scalability, and accuracy in many natural language processing tasks.'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What's the paper 1605.08386 about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b88b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
